{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# CPE342 - Karena Task3 V6: Full Stacking with Optuna & IterativeImputer\n",
                "\n",
                "## Upgrade Highlights\n",
                "- **Full Stacking**: Stacking applied to BOTH Classification (Stage 1) and Regression (Stage 2).\n",
                "- **Dual Optuna Tuning**: Automated hyperparameter optimization for both stages.\n",
                "- **Advanced Imputation**: Switched to `IterativeImputer` (MICE) for better accuracy.\n",
                "- **Robustness**: Separate preprocessors for each stage to prevent data leakage/errors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "import optuna\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
                "from sklearn.experimental import enable_iterative_imputer  # Explicitly enable\n",
                "from sklearn.impute import IterativeImputer, SimpleImputer\n",
                "from sklearn.pipeline import Pipeline, make_pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
                "from sklearn.ensemble import StackingRegressor, StackingClassifier, GradientBoostingRegressor\n",
                "from sklearn.linear_model import LassoCV, ElasticNetCV, Lasso, LogisticRegression\n",
                "from sklearn.kernel_ridge import KernelRidge\n",
                "from sklearn.base import clone\n",
                "\n",
                "from lightgbm import LGBMRegressor, LGBMClassifier\n",
                "import xgboost as xgb\n",
                "from xgboost import XGBRegressor, XGBClassifier\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "pd.set_option('display.max_columns', None)\n",
                "\n",
                "# FIX: Explicitly set XGBoost global verbosity to 0\n",
                "xgb.set_config(verbosity=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_data",
            "metadata": {},
            "source": [
                "## 1. Load Data & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "load_fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    train_df = pd.read_csv(\"Dataset/task3/train.csv\")\n",
                "    test_df = pd.read_csv(\"Dataset/task3/test.csv\")\n",
                "except FileNotFoundError:\n",
                "    train_df = pd.read_csv(\"train.csv\")\n",
                "    test_df = pd.read_csv(\"test.csv\")\n",
                "\n",
                "test_ids = test_df['id']\n",
                "\n",
                "def create_features(df):\n",
                "    df = df.copy()\n",
                "    df['spending_per_day'] = df['historical_spending'] / (df['account_age_days'] + 1e-6)\n",
                "    df['spending_per_transaction'] = df['historical_spending'] / (df['total_transactions'] + 1e-6)\n",
                "    df['prev_month_ratio'] = df['prev_month_spending'] / (df['historical_spending'] + 1e-6)\n",
                "    df['playtime_per_day'] = df['total_playtime_hours'] / (df['account_age_days'] + 1e-6)\n",
                "    df['playtime_per_session'] = df['total_playtime_hours'] / (df['sessions_per_week'] * 4.33 + 1e-6)\n",
                "    df['interaction_per_friend'] = df['social_interactions'] / (df['friend_count'] + 1e-6)\n",
                "    df['discount_purchase_ratio'] = df['purchases_on_discount'] / (df['total_transactions'] + 1e-6)\n",
                "    df['avg_discount_value'] = (df['discount_rate_used'] * df['purchases_on_discount']) / (df['total_transactions'] + 1e-6)\n",
                "    df['is_whale'] = (df['historical_spending'] > df['historical_spending'].quantile(0.95)).astype(int)\n",
                "    df['high_activity'] = (df['total_playtime_hours'] > df['total_playtime_hours'].quantile(0.90)).astype(int)\n",
                "    \n",
                "    skewed_cols = ['historical_spending', 'total_playtime_hours', 'friend_count']\n",
                "    for col in skewed_cols:\n",
                "        df[f'log_{col}'] = np.log1p(df[col])\n",
                "\n",
                "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "    return df\n",
                "\n",
                "train_df = create_features(train_df)\n",
                "test_df = create_features(test_df)\n",
                "\n",
                "TARGET = 'spending_30d'\n",
                "CATEGORICAL_FEATURES = [\n",
                "    'guild_membership', 'vip_status', 'is_premium_member', 'primary_game',\n",
                "    'games_played', 'cross_game_activity', 'platform', 'seasonal_spending_pattern',\n",
                "    'owns_limited_edition', 'tournament_participation', 'segment', 'is_whale', 'high_activity'\n",
                "]\n",
                "NUMERICAL_FEATURES = [\n",
                "    col for col in train_df.columns \n",
                "    if col not in [TARGET, 'id', 'player_id'] + CATEGORICAL_FEATURES\n",
                "]\n",
                "\n",
                "# Preprocessing with IterativeImputer\n",
                "numerical_transformer = Pipeline(steps=[\n",
                "    ('imputer', IterativeImputer(max_iter=10, random_state=0)), # MICE Imputation\n",
                "    ('scaler', RobustScaler())\n",
                "])\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                "])\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, NUMERICAL_FEATURES),\n",
                "        ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
                "    ],\n",
                "    remainder='passthrough'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stage1_tuning",
            "metadata": {},
            "source": [
                "## 2. Stage 1: Classification Tuning (Optuna)\n",
                "Optimizing LGBM and XGBoost for classification (Will Spend?)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "stage1_optuna",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tuning Classification Models...\n",
                        "Best LGBM Clf Params: {'n_estimators': 377, 'learning_rate': 0.04716650770151098, 'max_depth': 3, 'num_leaves': 91, 'subsample': 0.7858238978905429, 'colsample_bytree': 0.6814656253052074}\n",
                        "Best XGB Clf Params: {'n_estimators': 450, 'learning_rate': 0.03656346779074918, 'max_depth': 4, 'subsample': 0.9474764455162192, 'colsample_bytree': 0.853773046065345}\n"
                    ]
                }
            ],
            "source": [
                "y_class = (train_df[TARGET] > 0).astype(int)\n",
                "X = train_df[NUMERICAL_FEATURES + CATEGORICAL_FEATURES]\n",
                "\n",
                "# Clone preprocessor for Classification Tuning\n",
                "preprocessor_clf_tune = clone(preprocessor)\n",
                "X_processed = preprocessor_clf_tune.fit_transform(X)\n",
                "\n",
                "def objective_lgbm_clf(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'class_weight': 'balanced',\n",
                "        'n_jobs': -1,\n",
                "        'verbose': -1,\n",
                "        'random_state': 42\n",
                "    }\n",
                "    model = LGBMClassifier(**params)\n",
                "    return cross_val_score(model, X_processed, y_class, cv=3, scoring='roc_auc').mean()\n",
                "\n",
                "def objective_xgb_clf(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'scale_pos_weight': (len(y_class) - sum(y_class)) / sum(y_class),\n",
                "        'n_jobs': 4,\n",
                "        'verbosity': 0,\n",
                "        'random_state': 42\n",
                "    }\n",
                "    model = XGBClassifier(**params)\n",
                "    return cross_val_score(model, X_processed, y_class, cv=3, scoring='roc_auc').mean()\n",
                "\n",
                "print(\"Tuning Classification Models...\")\n",
                "study_lgbm_clf = optuna.create_study(direction='maximize')\n",
                "study_lgbm_clf.optimize(objective_lgbm_clf, n_trials=20)\n",
                "print(\"Best LGBM Clf Params:\", study_lgbm_clf.best_params)\n",
                "\n",
                "study_xgb_clf = optuna.create_study(direction='maximize')\n",
                "study_xgb_clf.optimize(objective_xgb_clf, n_trials=20)\n",
                "print(\"Best XGB Clf Params:\", study_xgb_clf.best_params)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stage1_stacking",
            "metadata": {},
            "source": [
                "## 3. Stage 1: Classification Stacking\n",
                "Combining optimized classifiers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "stage1_stacking_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Classification Stacking...\n",
                        "Stage 1 Complete.\n"
                    ]
                }
            ],
            "source": [
                "lgbm_clf_best = LGBMClassifier(**study_lgbm_clf.best_params, class_weight='balanced', n_jobs=-1, verbose=-1, random_state=42)\n",
                "xgb_clf_best = XGBClassifier(**study_xgb_clf.best_params, scale_pos_weight=(len(y_class) - sum(y_class)) / sum(y_class), n_jobs=4, verbosity=0, random_state=42)\n",
                "\n",
                "stack_clf = StackingClassifier(\n",
                "    estimators=[\n",
                "        ('lgbm', lgbm_clf_best),\n",
                "        ('xgb', xgb_clf_best),\n",
                "        ('lr', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
                "    ],\n",
                "    final_estimator=LogisticRegression(),\n",
                "    cv=5,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "# Clone preprocessor for Final Classification Pipeline\n",
                "preprocessor_clf_final = clone(preprocessor)\n",
                "\n",
                "clf_pipeline = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor_clf_final),\n",
                "    ('stacking_clf', stack_clf)\n",
                "])\n",
                "\n",
                "print(\"Training Classification Stacking...\")\n",
                "clf_pipeline.fit(X, y_class)\n",
                "print(\"Stage 1 Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stage2_tuning",
            "metadata": {},
            "source": [
                "## 4. Stage 2: Regression Tuning (Optuna)\n",
                "Optimizing Regressors for Spenders."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "stage2_optuna",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tuning Regression Models...\n",
                        "Best LGBM Reg Params: {'n_estimators': 230, 'learning_rate': 0.058772804989323815, 'max_depth': 7, 'num_leaves': 50, 'subsample': 0.6376909960035413, 'colsample_bytree': 0.9098169065185332, 'reg_alpha': 5.833461145808751e-08, 'reg_lambda': 1.5943068065572767e-05}\n",
                        "Best XGB Reg Params: {'n_estimators': 963, 'learning_rate': 0.054452797162615856, 'max_depth': 4, 'subsample': 0.589008253388284, 'colsample_bytree': 0.7933927699859022, 'reg_alpha': 0.0006349588798568599, 'reg_lambda': 0.6668490876906188}\n"
                    ]
                }
            ],
            "source": [
                "mask_spenders = train_df[TARGET] > 0\n",
                "X_spenders = train_df.loc[mask_spenders, NUMERICAL_FEATURES + CATEGORICAL_FEATURES]\n",
                "y_spenders_log = np.log1p(train_df.loc[mask_spenders, TARGET])\n",
                "\n",
                "# Clone preprocessor for Regression Tuning\n",
                "preprocessor_reg_tune = clone(preprocessor)\n",
                "X_spenders_processed = preprocessor_reg_tune.fit_transform(X_spenders)\n",
                "\n",
                "def objective_lgbm_reg(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
                "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
                "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
                "        'n_jobs': -1,\n",
                "        'verbose': -1,\n",
                "        'random_state': 42\n",
                "    }\n",
                "    model = LGBMRegressor(**params)\n",
                "    scores = cross_val_score(model, X_spenders_processed, y_spenders_log, cv=3, scoring='neg_root_mean_squared_error')\n",
                "    return -scores.mean()\n",
                "\n",
                "def objective_xgb_reg(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
                "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
                "        'n_jobs': 4,\n",
                "        'verbosity': 0,\n",
                "        'random_state': 42\n",
                "    }\n",
                "    model = XGBRegressor(**params)\n",
                "    scores = cross_val_score(model, X_spenders_processed, y_spenders_log, cv=3, scoring='neg_root_mean_squared_error')\n",
                "    return -scores.mean()\n",
                "\n",
                "print(\"Tuning Regression Models...\")\n",
                "study_lgbm_reg = optuna.create_study(direction='minimize')\n",
                "study_lgbm_reg.optimize(objective_lgbm_reg, n_trials=20)\n",
                "print(\"Best LGBM Reg Params:\", study_lgbm_reg.best_params)\n",
                "\n",
                "study_xgb_reg = optuna.create_study(direction='minimize')\n",
                "study_xgb_reg.optimize(objective_xgb_reg, n_trials=20)\n",
                "print(\"Best XGB Reg Params:\", study_xgb_reg.best_params)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stage2_stacking",
            "metadata": {},
            "source": [
                "## 5. Stage 2: Regression Stacking\n",
                "Combining optimized regressors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "stage2_stacking_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Regression Stacking...\n",
                        "Stage 2 Complete.\n"
                    ]
                }
            ],
            "source": [
                "lgbm_reg_best = LGBMRegressor(**study_lgbm_reg.best_params, n_jobs=-1, verbose=-1, random_state=42)\n",
                "xgb_reg_best = XGBRegressor(**study_xgb_reg.best_params, n_jobs=4, verbosity=0, random_state=42)\n",
                "\n",
                "stack_reg = StackingRegressor(\n",
                "    estimators=[\n",
                "        ('lasso', make_pipeline(RobustScaler(), LassoCV(cv=5, random_state=1))),\n",
                "        ('enet', make_pipeline(RobustScaler(), ElasticNetCV(cv=5, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], random_state=3))),\n",
                "        ('gboost', GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)),\n",
                "        ('xgb', xgb_reg_best),\n",
                "        ('lgbm', lgbm_reg_best)\n",
                "    ],\n",
                "    final_estimator=Lasso(alpha=0.0005, random_state=1),\n",
                "    n_jobs=-1,\n",
                "    passthrough=False\n",
                ")\n",
                "\n",
                "# Clone preprocessor for Final Regression Pipeline\n",
                "preprocessor_reg_final = clone(preprocessor)\n",
                "\n",
                "reg_pipeline = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor_reg_final),\n",
                "    ('stacking_reg', stack_reg)\n",
                "])\n",
                "\n",
                "print(\"Training Regression Stacking...\")\n",
                "reg_pipeline.fit(X_spenders, y_spenders_log)\n",
                "print(\"Stage 2 Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final_pred",
            "metadata": {},
            "source": [
                "## 6. Final Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "final_pred_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "X_test = test_df[NUMERICAL_FEATURES + CATEGORICAL_FEATURES]\n",
                "\n",
                "prob_spend = clf_pipeline.predict_proba(X_test)[:, 1]\n",
                "pred_log = reg_pipeline.predict(X_test)\n",
                "pred_amount = np.expm1(pred_log)\n",
                "\n",
                "final_predictions = prob_spend * pred_amount\n",
                "final_predictions[final_predictions < 0] = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a498cec0",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([   425.45942107,   1071.41500554, 170594.40368699, ...,\n",
                            "         4796.6913707 ,    407.19019557,   1587.82907701], shape=(25889,))"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "final_predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "c2257d96",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filling 'task3' column with predictions...\n",
                        "          id  task1  task2          task3  task4  task5\n",
                        "0   ANS00001    1.0      2     425.459421      1      0\n",
                        "1   ANS00002    0.0      0    1071.415006      3      0\n",
                        "2   ANS00003    1.0      0  170594.403687      3      1\n",
                        "3   ANS00004    0.0      0      81.560468      0      0\n",
                        "4   ANS00005    0.0      0     380.555185      3      0\n",
                        "5   ANS00006    1.0      2      80.807074      2      0\n",
                        "6   ANS00007    0.0      1     242.535729      1      0\n",
                        "7   ANS00008    0.0      0    9258.924874      3      0\n",
                        "8   ANS00009    1.0      0       6.733944      0      0\n",
                        "9   ANS00010    0.0      1      41.017704      3      0\n",
                        "10  ANS00011    0.0      2      10.504390      1      0\n",
                        "11  ANS00012    0.0      0     225.085861      3      0\n",
                        "12  ANS00013    0.0      0       0.804839      1      0\n",
                        "13  ANS00014    0.0      1    1737.155089      2      0\n",
                        "14  ANS00015    1.0      2      48.950250      4      0\n",
                        "15  ANS00016    0.0      0      65.034160      0      0\n",
                        "16  ANS00017    0.0      0     341.650566      3      0\n",
                        "17  ANS00018    0.0      0      62.980185      3      0\n",
                        "18  ANS00019    1.0      0       7.839411      0      0\n",
                        "19  ANS00020    1.0      0      22.449569      3      0\n"
                    ]
                }
            ],
            "source": [
                "submission_df = pd.read_csv(\"final_submission.csv\")\n",
                "target_column = 'task3'\n",
                "print(f\"Filling '{target_column}' column with predictions...\")\n",
                "submission_df[target_column] = final_predictions\n",
                "submission_df.to_csv(\"final_submission_task3_upgrade_fullstacking.csv\", index=False)\n",
                "print(submission_df.head(20))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
