{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f608d80b",
   "metadata": {},
   "source": [
    "# CPE342 - Karena Task4 Swin tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1d9784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memory Usage: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ตรวจสอบการ์ดจอทันที\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU not detected. Please check your PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745dbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_ROOT = \"Dataset/task4\"  \n",
    "    TRAIN_CSV = \"train.csv\"\n",
    "    VAL_CSV = \"val.csv\"\n",
    "    TEST_CSV = \"test_refined.csv\"\n",
    "    \n",
    "    TRAIN_DIR = \"train\"\n",
    "    VAL_DIR = \"val\"\n",
    "    TEST_DIR = \"test\"\n",
    "    \n",
    "    OUTPUT_DIR = \"output_swin_gpu\"\n",
    "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "    MODEL_NAME = \"swin_small_patch4_window7_224\"\n",
    "    IMG_SIZE = 224\n",
    "\n",
    "    BATCH_SIZE = 32  \n",
    "    \n",
    "    NUM_EPOCHS = 5\n",
    "    EARLY_STOPPING = 2\n",
    "    N_FOLDS = 5\n",
    "    \n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    SEED = 42\n",
    "\n",
    "    NUM_WORKERS = 0\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83745c",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d54dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # เปิด Benchmark ให้ CUDA หาอัลกอริทึมที่เร็วสุด\n",
    "\n",
    "def get_label_mapping(df):\n",
    "    label_col = \"label\" if \"label\" in df.columns else df.columns[1]\n",
    "    if df[label_col].dtype == 'O':\n",
    "        uniq = sorted(df[label_col].unique())\n",
    "        return {k:v for v,k in enumerate(uniq)}, len(uniq), label_col\n",
    "    return None, int(df[label_col].max())+1, label_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2b726",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19935f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(data):\n",
    "    if data == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((CFG.IMG_SIZE + 32, CFG.IMG_SIZE + 32)),\n",
    "            transforms.RandomResizedCrop(CFG.IMG_SIZE, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif data == 'valid':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ffcdc",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, label2idx=None, is_test=False, label_col=\"label\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.label2idx = label2idx\n",
    "        self.label_col = label_col\n",
    "        \n",
    "        # Detect Image Column\n",
    "        self.img_col = next((c for c in [\"file_name\", \"filename\", \"image\", \"id\"] if c in df.columns), df.columns[0])\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = str(row[self.img_col])\n",
    "        \n",
    "        # Search for file\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        if not os.path.exists(img_path):\n",
    "            for ext in ['.jpg', '.jpeg', '.png', '.JPG']:\n",
    "                if os.path.exists(img_path + ext):\n",
    "                    img_path += ext\n",
    "                    break\n",
    "            else:\n",
    "                img_path += '.jpg' # Fallback\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (CFG.IMG_SIZE, CFG.IMG_SIZE)) # Black image if error\n",
    "            \n",
    "        if self.transform: image = self.transform(image)\n",
    "        \n",
    "        if self.is_test: return image, img_name\n",
    "        \n",
    "        label = row[self.label_col]\n",
    "        if self.label2idx: label = self.label2idx[label]\n",
    "        return image, int(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e245ae5",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc0b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    model = timm.create_model(CFG.MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "    return model.to(CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2925e",
   "metadata": {},
   "source": [
    "## Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017482bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm # เพิ่มบรรทัดนี้หัวบนสุดของ cell หรือโค้ด\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    # ครอบ loader ด้วย tqdm เพื่อสร้าง Progress Bar\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        if scheduler: scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # อัปเดตตัวเลขท้ายหลอดโหลด real-time\n",
    "        current_acc = correct / total\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': current_acc})\n",
    "        \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aedb84",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed8d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: Train (31546, 3), Test (25889, 3)\n",
      "\n",
      "==================== Fold 1/5 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0833d5281fa4e90b2d5bf8556adc632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Train Loss 0.2590 Acc 0.8958 | Val Acc 0.9380 | 34696.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b9242dc57e45e8be72cf7115b79c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Train Loss 0.0828 Acc 0.9666 | Val Acc 0.9681 | 2719.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafd16f7f7404b2893f26bdcc79fafb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: Train Loss 0.0484 Acc 0.9802 | Val Acc 0.9791 | 3019.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370a615e23ec42d5bfd82e218e29f43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: Train Loss 0.0240 Acc 0.9893 | Val Acc 0.9834 | 2743.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c450ffae3e471aacaae12fe2b06b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: Train Loss 0.0172 Acc 0.9932 | Val Acc 0.9854 | 2648.2s\n",
      "\n",
      "==================== Fold 2/5 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72809338cfe42d297b9905ea5d5c0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Train Loss 0.2559 Acc 0.8963 | Val Acc 0.9442 | 2454.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b99b867ee484ba581b0be3c4c796c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Train Loss 0.0825 Acc 0.9681 | Val Acc 0.9729 | 2468.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501f410d2d414cbb923509ea2be6c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: Train Loss 0.0453 Acc 0.9829 | Val Acc 0.9738 | 3242.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffcfe91fc074893a631b76dfb37939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: Train Loss 0.0213 Acc 0.9911 | Val Acc 0.9822 | 3246.3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b2409674524023b474b6cb9848f7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: Train Loss 0.0149 Acc 0.9934 | Val Acc 0.9846 | 3250.0s\n",
      "\n",
      "==================== Fold 3/5 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c442601ea4a56b8480611d9644116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Train Loss 0.2501 Acc 0.8946 | Val Acc 0.9697 | 3141.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f381c0a87fef44c68915174f4cbf2b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fold_models, label2idx\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Run!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m trained_models, label_map = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, CFG.NUM_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m     57\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     loss_t, acc_t = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     loss_v, acc_v = valid_epoch(model, dl_val, criterion, CFG.DEVICE)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_t\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_t\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_v\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m     18\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     20\u001b[39m scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m scaler.update()\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler: scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\CPE342_Karena\\swin_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\CPE342_Karena\\swin_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\CPE342_Karena\\swin_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def run_training():\n",
    "    set_seed(CFG.SEED)\n",
    "    \n",
    "    # Load Data\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(CFG.DATA_ROOT, CFG.TRAIN_CSV))\n",
    "        test_df = pd.read_csv(os.path.join(CFG.DATA_ROOT, CFG.TEST_CSV))\n",
    "        print(f\"Data Loaded: Train {train_df.shape}, Test {test_df.shape}\")\n",
    "    except:\n",
    "        print(\"Error loading CSV. Please check paths in CFG.\")\n",
    "        return\n",
    "\n",
    "    label2idx, num_classes, label_col = get_label_mapping(train_df)\n",
    "    \n",
    "    # K-Fold\n",
    "    skf = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)\n",
    "    y_labels = train_df[label_col].map(label2idx).values if label2idx else train_df[label_col].values.astype(int)\n",
    "    \n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y_labels)):\n",
    "        print(f\"\\n{'='*20} Fold {fold+1}/{CFG.N_FOLDS} {'='*20}\")\n",
    "        \n",
    "        # Data Splitting\n",
    "        df_train = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Class Weights handling\n",
    "        y_train = y_labels[train_idx]\n",
    "        class_w = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights_tensor = torch.tensor(class_w, dtype=torch.float32).to(CFG.DEVICE)\n",
    "        \n",
    "        # Weighted Sampler\n",
    "        samples_weight = class_w[y_train]\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "        # Datasets & Loaders\n",
    "        ds_train = GameDataset(df_train, os.path.join(CFG.DATA_ROOT, CFG.TRAIN_DIR), get_transforms('train'), label2idx, label_col=label_col)\n",
    "        ds_val = GameDataset(df_val, os.path.join(CFG.DATA_ROOT, CFG.TRAIN_DIR), get_transforms('valid'), label2idx, label_col=label_col)\n",
    "        \n",
    "        dl_train = DataLoader(ds_train, batch_size=CFG.BATCH_SIZE, sampler=sampler, \n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=True) # pin_memory for GPU\n",
    "        dl_val = DataLoader(ds_val, batch_size=CFG.BATCH_SIZE, shuffle=False, \n",
    "                            num_workers=CFG.NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        # Init Model\n",
    "        model = create_model(num_classes)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.NUM_EPOCHS*len(dl_train))\n",
    "        \n",
    "        # Train Loop\n",
    "        best_acc = 0\n",
    "        patience = 0\n",
    "        \n",
    "        for epoch in range(1, CFG.NUM_EPOCHS+1):\n",
    "            start = time.time()\n",
    "            loss_t, acc_t = train_epoch(model, dl_train, criterion, optimizer, scheduler, CFG.DEVICE)\n",
    "            loss_v, acc_v = valid_epoch(model, dl_val, criterion, CFG.DEVICE)\n",
    "            \n",
    "            print(f\"Ep {epoch}: Train Loss {loss_t:.4f} Acc {acc_t:.4f} | Val Acc {acc_v:.4f} | {time.time()-start:.1f}s\")\n",
    "            \n",
    "            if acc_v > best_acc:\n",
    "                best_acc = acc_v\n",
    "                torch.save(model.state_dict(), os.path.join(CFG.OUTPUT_DIR, f\"swin_fold{fold}.pth\"))\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= CFG.EARLY_STOPPING:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        fold_models.append(os.path.join(CFG.OUTPUT_DIR, f\"swin_fold{fold}.pth\"))\n",
    "        \n",
    "    return fold_models, label2idx\n",
    "\n",
    "# Run!\n",
    "trained_models, label_map = run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf00fee",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b82b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Unfreezing model for Fine-Tuning ---\n"
     ]
    }
   ],
   "source": [
    "def inference(models_path, label2idx):\n",
    "    test_df = pd.read_csv(os.path.join(CFG.DATA_ROOT, CFG.TEST_CSV))\n",
    "    ds_test = GameDataset(test_df, os.path.join(CFG.DATA_ROOT, CFG.TEST_DIR), get_transforms('valid'), is_test=True)\n",
    "    dl_test = DataLoader(ds_test, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    num_classes = len(label2idx) if label2idx else test_df['label'].nunique() # Check logic\n",
    "    if not num_classes: num_classes = 5 # Default fallback\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    for path in models_path:\n",
    "        print(f\"Predicting with {path}...\")\n",
    "        model = create_model(num_classes)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        \n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for img, _ in dl_test:\n",
    "                out = model(img.to(CFG.DEVICE))\n",
    "                probs.append(torch.softmax(out, dim=1).cpu().numpy())\n",
    "        all_probs.append(np.concatenate(probs))\n",
    "        \n",
    "    # Average Predictions\n",
    "    avg_probs = np.mean(all_probs, axis=0)\n",
    "    preds = avg_probs.argmax(1)\n",
    "    \n",
    "    if label2idx:\n",
    "        inv_map = {v:k for k,v in label2idx.items()}\n",
    "        final_preds = [inv_map[p] for p in preds]\n",
    "    else:\n",
    "        final_preds = preds\n",
    "\n",
    "# Run Inference if training finished\n",
    "if 'trained_models' in locals() and trained_models:\n",
    "    inference(trained_models, label_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
