{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072b6c42",
   "metadata": {},
   "source": [
    "# CPE342 - Karena Task1 V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad2abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import fbeta_score, make_scorer, precision_recall_curve\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49d423",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b0fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (99872, 34)\n",
      "Test shape: (25889, 33)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Dataset/task1/train.csv\")\n",
    "test_df = pd.read_csv(\"Dataset/task1/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9ea98",
   "metadata": {},
   "source": [
    "## Remove Missing Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aca4ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2124 rows with missing target\n",
      "\n",
      "Class distribution:\n",
      "is_cheater\n",
      "0.0    63619\n",
      "1.0    34129\n",
      "Name: count, dtype: int64\n",
      "is_cheater\n",
      "0.0    0.650847\n",
      "1.0    0.349153\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target = 'is_cheater'\n",
    "if train_df[target].isnull().any():\n",
    "    print(f\"Removing {train_df[target].isnull().sum()} rows with missing target\")\n",
    "    train_df = train_df.dropna(subset=[target])\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df[target].value_counts())\n",
    "print(train_df[target].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f2ee0",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d1df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Creating strategic features (V4 Enhanced)...\n",
      "Enhanced train shape: (97748, 49)\n"
     ]
    }
   ],
   "source": [
    "def create_smart_features(df):\n",
    "    \"\"\"Create meaningful features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- PREPROCESSING: Outlier Handling ---\n",
    "    # Clip reaction time to be at least 1ms\n",
    "    if 'reaction_time_ms' in df.columns:\n",
    "        df['reaction_time_ms'] = df['reaction_time_ms'].clip(lower=1)\n",
    "        \n",
    "    # 1. Suspicious skill indicators\n",
    "    df['kd_headshot_product'] = df['kill_death_ratio'] * df['headshot_percentage']\n",
    "    \n",
    "    # 2. New account with high performance (red flag!)\n",
    "    df['performance_per_day'] = (\n",
    "        df['kill_death_ratio'] + df['win_rate'] + df['headshot_percentage']\n",
    "    ) / (df['account_age_days'] + 1)\n",
    "    \n",
    "    # 3. Social isolation\n",
    "    df['report_friend_ratio'] = df['reports_received'] / (df['friend_network_size'] + 1)\n",
    "    \n",
    "    # 4. Superhuman accuracy with fast reaction\n",
    "    df['accuracy_reaction_ratio'] = df['accuracy_score'] / (df['reaction_time_ms'] + 1)\n",
    "    \n",
    "    # 5. Unnatural consistency\n",
    "    df['consistency_kd_product'] = df['kill_consistency'] * df['kill_death_ratio']\n",
    "    \n",
    "    # 6. Combat dominance\n",
    "    df['combat_score'] = (\n",
    "        df['first_blood_rate'] * df['clutch_success_rate'] * df['damage_per_round']\n",
    "    )\n",
    "    \n",
    "    # --- NEW FEATURES ---\n",
    "    # 7. Skill Consistency (User Suggestion)\n",
    "    df['skill_consistency'] = df['game_sense_score'] / (df['accuracy_score'] + 0.001)\n",
    "    \n",
    "    # 8. Impact Score\n",
    "    df['impact_score'] = df['kill_death_ratio'] * df['win_rate']\n",
    "    \n",
    "    # 9. Precision Index\n",
    "    df['precision_index'] = df['headshot_percentage'] * df['accuracy_score']\n",
    "    \n",
    "    # 10. Utility Efficiency\n",
    "    df['utility_efficiency'] = df['utility_usage_rate'] * df['damage_per_round']\n",
    "    \n",
    "    # 11. Suspicion Score (Composite)\n",
    "    df['suspicion_score'] = (df['kill_death_ratio'] * df['headshot_percentage']) / (df['account_age_days'] + 1)\n",
    "    \n",
    "    # --- V4 IMPROVEMENTS ---\n",
    "    # 12. Aim-Movement Mismatch\n",
    "    # Cheaters often have perfect aim but poor movement\n",
    "    df['aim_movement_mismatch'] = df['aiming_smoothness'] / (df['movement_pattern_score'] + 0.01)\n",
    "    \n",
    "    # 13. Report Intensity\n",
    "    # Reports normalized by playtime and account age\n",
    "    df['report_intensity'] = df['reports_received'] / (df['avg_session_length_min'] * df['sessions_per_day'] * df['account_age_days'] + 1)\n",
    "    \n",
    "    # 14. Rapid Progression\n",
    "    df['rapid_progression'] = df['level'] / (df['account_age_days'] + 1)\n",
    "    \n",
    "    # 15. Hardware Volatility\n",
    "    df['hardware_volatility'] = df['device_changes_count'] / (df['account_age_days'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\nâœ¨ Creating strategic features (V4 Enhanced)...\")\n",
    "train_enhanced = create_smart_features(train_df)\n",
    "test_enhanced = create_smart_features(test_df)\n",
    "print(f\"Enhanced train shape: {train_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19df2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prepare Features\n",
    "features = [col for col in train_enhanced.columns \n",
    "           if col not in ['id', 'player_id', 'is_cheater']]\n",
    "\n",
    "X_train = train_enhanced[features]\n",
    "y_train = train_enhanced[target]\n",
    "X_test = test_enhanced[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a65bfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Smart imputation...\n",
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: âœ¨ IMPROVEMENT 2 - Better Imputation Strategy\n",
    "# à¹ƒà¸Šà¹‰ mean à¸ªà¸³à¸«à¸£à¸±à¸š features à¸—à¸µà¹ˆà¸¡à¸µ missing à¸™à¹‰à¸­à¸¢, median à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸µà¹ˆà¸¡à¸µà¸¡à¸²à¸\n",
    "def smart_imputation(X_train, X_test):\n",
    "    \"\"\"Impute à¸•à¸²à¸¡ distribution à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° feature\"\"\"\n",
    "    \n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_test_imputed = X_test.copy()\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        missing_rate = X_train[col].isnull().sum() / len(X_train)\n",
    "        \n",
    "        if missing_rate > 0:\n",
    "            # à¸–à¹‰à¸² missing à¸¡à¸²à¸ (>10%) à¹ƒà¸Šà¹‰ median à¸—à¸µà¹ˆà¹€à¸ªà¸–à¸µà¸¢à¸£à¸à¸§à¹ˆà¸²\n",
    "            if missing_rate > 0.1:\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "            else:\n",
    "                # à¸–à¹‰à¸² missing à¸™à¹‰à¸­à¸¢ à¹ƒà¸Šà¹‰ mean à¸—à¸µà¹ˆà¸£à¸±à¸à¸©à¸²à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "            \n",
    "            X_train_imputed[col] = imputer.fit_transform(X_train[[col]])\n",
    "            X_test_imputed[col] = imputer.transform(X_test[[col]])\n",
    "    \n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "print(\"\\nâœ¨ Smart imputation...\")\n",
    "X_train_imputed, X_test_imputed = smart_imputation(X_train, X_test)\n",
    "print(f\"Missing values after imputation: {X_train_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ec4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Scale pos weight: 1.86\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Calculate Class Weight\n",
    "scale_pos_weight = (y_train == 0).sum() / y_train.sum()\n",
    "print(f\"\\nâš–ï¸ Scale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2fbcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Optimized hyperparameters:\n",
      "  n_estimators: 500\n",
      "  learning_rate: 0.02\n",
      "  num_leaves: 40\n",
      "  max_depth: 8\n",
      "  min_child_samples: 25\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 0.8\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 0.1\n",
      "  scale_pos_weight: 1.8640745407131765\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: âœ¨ IMPROVEMENT 3 - Optimized Hyperparameters\n",
    "# à¸‚à¸¢à¸²à¸¢ search space à¹à¸¥à¸°à¹€à¸žà¸´à¹ˆà¸¡ regularization\n",
    "best_params = {\n",
    "    'n_estimators': 500,  # à¹€à¸žà¸´à¹ˆà¸¡à¸ˆà¸²à¸ 100-400\n",
    "    'learning_rate': 0.02,  # à¸¥à¸”à¸¥à¸‡à¹€à¸žà¸·à¹ˆà¸­à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‚à¸¶à¹‰à¸™\n",
    "    'num_leaves': 40,  # à¹€à¸žà¸´à¹ˆà¸¡à¸ˆà¸²à¸ 31-50\n",
    "    'max_depth': 8,  # à¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸¥à¸¶à¸à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ overfit\n",
    "    'min_child_samples': 25,  # à¹€à¸žà¸´à¹ˆà¸¡ regularization\n",
    "    'subsample': 0.8,  # bagging\n",
    "    'colsample_bytree': 0.8,  # feature sampling\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 0.1,  # L2 regularization\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"\\nâœ¨ Optimized hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    if key not in ['random_state', 'verbose']:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feature_selection_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Performing Feature Selection...\n",
      "Dropping 0 features with 0 importance: []\n",
      "Selected 46 features.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.5: âœ¨ IMPROVEMENT 6 - Feature Selection\n",
    "def select_features(X, y):\n",
    "    \"\"\"Select important features using LightGBM\"\"\"\n",
    "    print(\"\\nðŸ” Performing Feature Selection...\")\n",
    "    \n",
    "    # Train a quick LGBM to get feature importance\n",
    "    lgbm = lgb.LGBMClassifier(n_estimators=500, random_state=42, verbose=-1)\n",
    "    lgbm.fit(X, y)\n",
    "    \n",
    "    importances = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': lgbm.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Keep features with importance > 0 (or top N)\n",
    "    # Here we drop features with 0 importance or very low relative importance\n",
    "    zero_imp_features = importances[importances['importance'] == 0]['feature'].tolist()\n",
    "    print(f\"Dropping {len(zero_imp_features)} features with 0 importance: {zero_imp_features}\")\n",
    "    \n",
    "    selected_feats = importances[importances['importance'] > 0]['feature'].tolist()\n",
    "    return selected_feats\n",
    "\n",
    "selected_features = select_features(X_train_imputed, y_train)\n",
    "X_train_selected = X_train_imputed[selected_features]\n",
    "X_test_selected = X_test_imputed[selected_features]\n",
    "print(f\"Selected {len(selected_features)} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148c8d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Weighted Ensemble with SMOTE...\n",
      "Starting 5-Fold Cross-Validation with Weighted Ensemble...\n",
      "Fold 1: Best F2 = 0.8367 (Weights: L=0.33, X=0.33, C=0.33)\n",
      "Fold 2: Best F2 = 0.8368 (Weights: L=0.33, X=0.33, C=0.33)\n",
      "Fold 3: Best F2 = 0.8365 (Weights: L=0.33, X=0.33, C=0.33)\n",
      "Fold 4: Best F2 = 0.8336 (Weights: L=0.33, X=0.33, C=0.33)\n",
      "Fold 5: Best F2 = 0.8396 (Weights: L=0.33, X=0.33, C=0.33)\n",
      "\n",
      "ðŸ“Š Mean F2-score: 0.8366\n",
      "âš–ï¸  Average Weights: LGBM=0.33, XGB=0.33, Cat=0.33\n",
      "ðŸŽ¯ Average Optimal Threshold: 0.1827\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: âœ¨ IMPROVEMENT 4 - Weighted Ensemble with SMOTE & Threshold Tuning\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def train_weighted_ensemble(X, y, n_splits=5):\n",
    "    \"\"\"Train Ensemble (LGBM, XGB, CatBoost) and find optimal weights\"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    models = []\n",
    "    cv_scores = []\n",
    "    best_thresholds = []\n",
    "    fold_weights = []\n",
    "    \n",
    "    # Base Models (Optimized)\n",
    "    lgbm_params = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.02,\n",
    "        'num_leaves': 40,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "    \n",
    "    cat_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.02,\n",
    "        'depth': 8,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting {n_splits}-Fold Cross-Validation with Weighted Ensemble...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # 1. Apply SMOTE only on Training Data\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # 2. Train Models\n",
    "        lgbm = lgb.LGBMClassifier(**lgbm_params)\n",
    "        lgbm.fit(X_resampled, y_resampled, eval_set=[(X_fold_val, y_fold_val)], \n",
    "                 callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        xgb_model.fit(X_resampled, y_resampled, eval_set=[(X_fold_val, y_fold_val)], \n",
    "                      verbose=False)\n",
    "        \n",
    "        cat_model = CatBoostClassifier(**cat_params)\n",
    "        cat_model.fit(X_resampled, y_resampled, eval_set=[(X_fold_val, y_fold_val)], \n",
    "                      early_stopping_rounds=50)\n",
    "        \n",
    "        # 3. Get Predictions\n",
    "        p1 = lgbm.predict_proba(X_fold_val)[:, 1]\n",
    "        p2 = xgb_model.predict_proba(X_fold_val)[:, 1]\n",
    "        p3 = cat_model.predict_proba(X_fold_val)[:, 1]\n",
    "        \n",
    "        # 4. Optimize Weights\n",
    "        def f2_loss(weights):\n",
    "            w1, w2, w3 = weights\n",
    "            # Normalize\n",
    "            total = w1 + w2 + w3\n",
    "            if total == 0: return 1\n",
    "            w1, w2, w3 = w1/total, w2/total, w3/total\n",
    "            \n",
    "            avg_prob = w1*p1 + w2*p2 + w3*p3\n",
    "            \n",
    "            # Find best threshold for these weights\n",
    "            precisions, recalls, thresholds = precision_recall_curve(y_fold_val, avg_prob)\n",
    "            f2_scores = (5 * precisions * recalls) / (4 * precisions + recalls + 1e-10)\n",
    "            return -np.max(f2_scores) # Minimize negative F2\n",
    "            \n",
    "        res = minimize(f2_loss, [1, 1, 1], bounds=[(0,1), (0,1), (0,1)], method='SLSQP')\n",
    "        best_w = res.x / np.sum(res.x)\n",
    "        \n",
    "        # 5. Calculate Score with Best Weights\n",
    "        avg_prob = best_w[0]*p1 + best_w[1]*p2 + best_w[2]*p3\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_fold_val, avg_prob)\n",
    "        f2_scores = (5 * precisions * recalls) / (4 * precisions + recalls + 1e-10)\n",
    "        best_idx = np.argmax(f2_scores)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "        best_score = f2_scores[best_idx]\n",
    "        \n",
    "        print(f\"Fold {fold}: Best F2 = {best_score:.4f} (Weights: L={best_w[0]:.2f}, X={best_w[1]:.2f}, C={best_w[2]:.2f})\")\n",
    "        \n",
    "        models.append((lgbm, xgb_model, cat_model))\n",
    "        cv_scores.append(best_score)\n",
    "        best_thresholds.append(best_threshold)\n",
    "        fold_weights.append(best_w)\n",
    "    \n",
    "    avg_threshold = np.mean(best_thresholds)\n",
    "    avg_weights = np.mean(fold_weights, axis=0)\n",
    "    print(f\"\\nðŸ“Š Mean F2-score: {np.mean(cv_scores):.4f}\")\n",
    "    print(f\"âš–ï¸  Average Weights: LGBM={avg_weights[0]:.2f}, XGB={avg_weights[1]:.2f}, Cat={avg_weights[2]:.2f}\")\n",
    "    print(f\"ðŸŽ¯ Average Optimal Threshold: {avg_threshold:.4f}\")\n",
    "    \n",
    "    return models, avg_threshold, avg_weights\n",
    "\n",
    "print(\"\\nðŸš€ Training Weighted Ensemble with SMOTE...\")\n",
    "models, best_threshold, best_weights = train_weighted_ensemble(\n",
    "    X_train_selected, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f532405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”® Making weighted ensemble predictions...\n",
      "\n",
      "ðŸ“Š Feature Importance (Average LGBM):\n",
      "                    feature  importance\n",
      "4          reports_received      3754.2\n",
      "0       friend_network_size      3577.2\n",
      "1          account_age_days      3394.2\n",
      "2       crosshair_placement      1577.8\n",
      "5          game_sense_score      1410.2\n",
      "24      report_friend_ratio      1332.0\n",
      "6                     level      1320.4\n",
      "45     device_changes_count      1096.8\n",
      "33      kd_headshot_product      1080.2\n",
      "3   level_progression_speed      1003.8\n",
      "31          precision_index       826.8\n",
      "18      headshot_percentage       721.0\n",
      "7       spray_control_score       716.4\n",
      "8         aiming_smoothness       713.0\n",
      "41          suspicion_score       638.2\n",
      "\n",
      "ðŸ’¾ Saving model...\n",
      "Model saved to Model/anti_cheat_weighted_V5\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: âœ¨ IMPROVEMENT 5 - Weighted Ensemble Prediction\n",
    "def ensemble_predict_final(models, X, weights, threshold=0.5):\n",
    "    \"\"\"Predict using all CV models and weighted average\"\"\"\n",
    "    final_probs = np.zeros(len(X))\n",
    "    \n",
    "    for (lgbm, xgb_model, cat_model) in models:\n",
    "        p1 = lgbm.predict_proba(X)[:, 1]\n",
    "        p2 = xgb_model.predict_proba(X)[:, 1]\n",
    "        p3 = cat_model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Weighted Average for this fold\n",
    "        fold_prob = weights[0]*p1 + weights[1]*p2 + weights[2]*p3\n",
    "        final_probs += fold_prob\n",
    "    \n",
    "    # Average across all folds\n",
    "    final_probs /= len(models)\n",
    "    \n",
    "    # Apply optimized threshold\n",
    "    return (final_probs >= threshold).astype(int)\n",
    "\n",
    "print(\"\\nðŸ”® Making weighted ensemble predictions...\")\n",
    "test_predictions = ensemble_predict_final(models, X_test_selected, best_weights, threshold=best_threshold)\n",
    "\n",
    "# Feature Importance (Average from LGBM models)\n",
    "print(\"\\nðŸ“Š Feature Importance (Average LGBM):\")\n",
    "importances = np.zeros(len(X_train_selected.columns))\n",
    "for (lgbm, _, _) in models:\n",
    "    importances += lgbm.feature_importances_\n",
    "importances /= len(models)\n",
    "\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': X_train_selected.columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_imp.head(15))\n",
    "\n",
    "# Save Model\n",
    "print(\"\\nðŸ’¾ Saving model...\")\n",
    "model_filename = 'Model/anti_cheat_weighted_V5'\n",
    "joblib.dump({\n",
    "    'models': models,\n",
    "    'feature_names': list(X_train_selected.columns),\n",
    "    'best_threshold': best_threshold,\n",
    "    'best_weights': best_weights\n",
    "}, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb220d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0], shape=(25889,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b06cb4",
   "metadata": {},
   "source": [
    "## Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79893ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(\"final_submission.csv\")\n",
    "target_column = 'task1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15564222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling 'task1' column with predictions...\n",
      "          id  task1  task2          task3  task4  task5\n",
      "0   ANS00001      1      2     517.156944      1      0\n",
      "1   ANS00002      0      0    1413.611996      0      0\n",
      "2   ANS00003      1      0  158344.753397      3      1\n",
      "3   ANS00004      0      0     117.963622      4      0\n",
      "4   ANS00005      1      0     101.542132      4      0\n",
      "5   ANS00006      1      2     609.682928      3      0\n",
      "6   ANS00007      1      1       0.000000      1      0\n",
      "7   ANS00008      1      0   17357.013216      0      0\n",
      "8   ANS00009      1      0       0.000000      3      0\n",
      "9   ANS00010      0      1       0.000000      0      0\n",
      "10  ANS00011      1      2     128.685354      0      0\n",
      "11  ANS00012      0      0     432.884720      3      0\n",
      "12  ANS00013      0      0      25.834546      2      0\n",
      "13  ANS00014      0      1    1670.033716      0      0\n",
      "14  ANS00015      1      2       0.000000      4      0\n",
      "15  ANS00016      0      0       0.000000      2      0\n",
      "16  ANS00017      1      0     354.180088      4      0\n",
      "17  ANS00018      0      0       0.000000      3      0\n",
      "18  ANS00019      1      0     139.534492      4      0\n",
      "19  ANS00020      1      0     137.653094      1      0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filling '{target_column}' column with predictions...\")\n",
    "submission_df[target_column] = test_predictions\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "print(submission_df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
